{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/nigel/anaconda3/envs/tensor_keras_portfolio/lib/python3.7/site-packages/tflearn/helpers/summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/nigel/anaconda3/envs/tensor_keras_portfolio/lib/python3.7/site-packages/tflearn/helpers/trainer.py:25: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/nigel/anaconda3/envs/tensor_keras_portfolio/lib/python3.7/site-packages/tflearn/collections.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/nigel/anaconda3/envs/tensor_keras_portfolio/lib/python3.7/site-packages/tflearn/config.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/nigel/anaconda3/envs/tensor_keras_portfolio/lib/python3.7/site-packages/tflearn/config.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/nigel/anaconda3/envs/tensor_keras_portfolio/lib/python3.7/site-packages/tflearn/config.py:131: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Imports Complete\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "import os\n",
    "import numpy as np\n",
    "from utils.data import read_stock_history, normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from model.ddpg.actor import ActorNetwork\n",
    "from model.ddpg.critic import CriticNetwork\n",
    "from model.ddpg.ddpg import DDPG\n",
    "from model.ornstein_uhlenbeck import OrnsteinUhlenbeckActionNoise\n",
    "from model.td3.actor import TD3ActorNetwork\n",
    "from model.td3.critic import TD3CriticNetwork\n",
    "from model.td3.td3 import TD3\n",
    "from stock_trading import StockActor, StockCritic, TD3StockActor, TD3StockCritic, obs_normalizer, get_model_path, get_result_path, test_model, get_variable_scope, test_model_multiple\n",
    "from environment.portfolio import PortfolioEnv, MultiActionPortfolioEnv, max_drawdown, sharpe, sortino, create_close_dataframe, convert_prices\n",
    "import tflearn\n",
    "from stock_trading import test_portfolio_selection, test_portfolio_selection_multiple, plot_weights, plot_portfolio_values, results_table, results_table_row, returns_from_cumulative, test_with_given_weights\n",
    "\n",
    "print(\"Imports Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common settings\n",
    "batch_size = 64\n",
    "action_bound = 1.\n",
    "tau = 1e-3\n",
    "\n",
    "models = []\n",
    "model_names = []\n",
    "framework_lst = ['TD3'] #['DDPG','TD3']\n",
    "window_length_lst = [3] #[3, 7, 11, 14]\n",
    "maximum_window = max(window_length_lst)\n",
    "predictor_type_lst = ['lstm']\n",
    "use_batch_norm = True\n",
    "log_return = True\n",
    "technical_indicators_flag = False\n",
    "load_technical_indicators = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: nyse_n\n",
      "Assets: ['ahp', 'alcoa', 'amer_brands', 'coke', 'comm_metals', 'dow_chem', 'Dupont', 'ford', 'ge', 'gm', 'hp', 'ibm', 'ingersoll', 'jnj', 'kimb-clark', 'kin_ark', 'Kodak', 'merck', 'mmm', 'morris', 'p_and_g', 'schlum', 'sher_will']\n",
      "Stock History Shape: (23, 6430, 2)\n",
      "Full Stock History Date Range: 02/01/1985 -> 30/06/2010\n"
     ]
    }
   ],
   "source": [
    "# Stock History\n",
    "dataset_name = 'nyse_n'\n",
    "history, assets, date_list = read_stock_history(filepath='utils/datasets/{}.h5'.format(dataset_name))\n",
    "history = history[:, :, :4]\n",
    "nb_classes = len(history) + 1\n",
    "print(\"Dataset: {}\".format(dataset_name))\n",
    "print(\"Assets: {}\".format(assets))\n",
    "print(\"Stock History Shape: {}\".format(history.shape))\n",
    "print(\"Full Stock History Date Range: {} -> {}\".format(date_list[0], date_list[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Date Range: 07/01/1985 -> 02/11/2006 (5507 Steps)\n",
      "Testing Date Range: 03/11/2006 -> 29/06/2010 (917 Steps)\n"
     ]
    }
   ],
   "source": [
    "# Training/Testing Date Range\n",
    "full_length = len(date_list)\n",
    "train_test_ratio = 6/7\n",
    "train_start_date = date_list[maximum_window]\n",
    "train_end_date = date_list[(int)(full_length * train_test_ratio)-1]\n",
    "test_start_date = date_list[(int)(full_length * train_test_ratio)]\n",
    "test_end_date = date_list[full_length-2]\n",
    "print(\"Training Date Range: {} -> {} ({} Steps)\".format(train_start_date, train_end_date, \n",
    "                                                    (int)(date_list.index(train_end_date) - date_list.index(train_start_date))))\n",
    "print(\"Testing Date Range: {} -> {} ({} Steps)\".format(test_start_date, test_end_date, \n",
    "                                                    (int)(date_list.index(test_end_date) - date_list.index(test_start_date))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Technical Indicators\n",
    "def GenerateTechnicalIndicators(window_length, debug = False):\n",
    "    if debug:\n",
    "        print('history.shape: {}'.format(history.shape))\n",
    "\n",
    "    # Check if just close is fed or full dataset\n",
    "    if history.shape[2] == 4:\n",
    "        # Get Close\n",
    "        history_close = history[:, :, 3]\n",
    "    elif history.shape[2] == 2:\n",
    "        # Assume the one sent is the close price\n",
    "        history_close = history[:, :, 1]\n",
    "\n",
    "    if debug:\n",
    "        print('history_close.shape: {}'.format(history_close.shape))\n",
    "\n",
    "    # Add Technical Indicators to be included in state\n",
    "    if technical_indicators_flag:\n",
    "        technical_indicator_history = []\n",
    "\n",
    "        # Close Price DataFrame for on-line Portfolio Selection\n",
    "        temp_close_df = create_close_dataframe(history, assets, date_list)\n",
    "        temp_close_df = convert_prices(temp_close_df, 'raw', True)    \n",
    "        if debug:\n",
    "            print('temp_close_df.shape: {}'.format(temp_close_df.shape))\n",
    "\n",
    "        rmr_moving_average_df = full_rmr_moving_average(temp_close_df, window_length)\n",
    "        for i in range(len(assets)):\n",
    "            # Create List\n",
    "            ti = []\n",
    "            #ti.append([olmar_moving_average(temp_close_df[assets[i]], window_length)]) # Based on OLMAR\n",
    "            ti.append([rmr_moving_average_df[assets[i]]]) # Based on RMR\n",
    "\n",
    "            # Turn to Array\n",
    "            ti = np.vstack(ti)\n",
    "\n",
    "            ti_reshaped = []\n",
    "            for j in range(temp_close_df.shape[0]):\n",
    "                ti_reshaped.append(ti[:, j])   \n",
    "\n",
    "            technical_indicator_history.append(ti_reshaped)\n",
    "\n",
    "        technical_indicator_history = np.array(technical_indicator_history)    \n",
    "        if debug:\n",
    "            print('technical_indicators.shape: {}'.format(technical_indicator_history.shape)) \n",
    "    else:\n",
    "        technical_indicator_history = None\n",
    "        \n",
    "    return technical_indicator_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if technical_indicators_flag:\n",
    "    if load_technical_indicators:\n",
    "        # Load Technical Indicators dataset\n",
    "        ti_dict = np.load('technical_indicators/dictionaries/ti_dict-{}-rmr.npy'.format(dataset_name),allow_pickle='TRUE').item()\n",
    "    else:\n",
    "        ti_dict = {}\n",
    "        for window_length in window_length_lst:\n",
    "            technical_indicator_history = GenerateTechnicalIndicators(window_length)\n",
    "            ti_dict[window_length] = technical_indicator_history\n",
    "        np.save('technical_indicators/dictionaries/ti_dict-{}-rmr.npy'.format(dataset_name), ti_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2acee37acd55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{}_{}_window_{}_predictor_{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframework\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictor_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mmodel_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mtflearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_training_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# instantiate environment\n",
    "for framework in framework_lst:\n",
    "    for window_length in window_length_lst:\n",
    "        for predictor_type in predictor_type_lst:\n",
    "            if technical_indicators_flag:\n",
    "                name = '{}_{}_window_{}_predictor_{}_rmr-pred'.format(dataset_name, framework, window_length, predictor_type)\n",
    "            else:\n",
    "                name = '{}_{}_window_{}_predictor_{}'.format(dataset_name, framework, window_length, predictor_type)\n",
    "            model_names.append(name)\n",
    "            tf.reset_default_graph()\n",
    "            sess = tf.Session()\n",
    "            tflearn.config.init_training_mode()\n",
    "            action_dim = [nb_classes]\n",
    "            if technical_indicators_flag:\n",
    "                technical_indicator_history = ti_dict[window_length]\n",
    "                state_dim = [nb_classes, window_length+technical_indicator_history.shape[2]]\n",
    "            else:\n",
    "                state_dim = [nb_classes, window_length]\n",
    "\n",
    "            variable_scope = get_variable_scope(dataset_name, framework, window_length, predictor_type, \n",
    "                                                use_batch_norm, technical_indicators_flag)\n",
    "\n",
    "            with tf.variable_scope(variable_scope):\n",
    "\n",
    "                if(framework == 'DDPG'):\n",
    "                    actor = StockActor(sess, state_dim, action_dim, action_bound, 1e-4, tau, batch_size, \n",
    "                                       predictor_type, use_batch_norm)\n",
    "                    critic = StockCritic(sess=sess, state_dim=state_dim, action_dim=action_dim, tau=1e-3,\n",
    "                             learning_rate=1e-3, num_actor_vars=actor.get_num_trainable_vars(), \n",
    "                             predictor_type=predictor_type, use_batch_norm=use_batch_norm)\n",
    "                    actor_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(action_dim))\n",
    "\n",
    "                    model_save_path = get_model_path(dataset_name, framework, window_length, predictor_type, use_batch_norm, technical_indicators_flag)\n",
    "                    summary_path = get_result_path(dataset_name, framework, window_length, predictor_type, use_batch_norm, technical_indicators_flag)\n",
    "\n",
    "                    ddpg_model = DDPG(None, sess, actor, critic, actor_noise, obs_normalizer=obs_normalizer,\n",
    "                                      log_return=log_return, config_file='config/stock.json', \n",
    "                                      model_save_path=model_save_path, summary_path=summary_path)\n",
    "                    ddpg_model.initialize(load_weights=True, verbose=True)\n",
    "                    models.append(ddpg_model)\n",
    "\n",
    "                elif(framework == 'TD3'):\n",
    "                    actor = TD3StockActor(sess, state_dim, action_dim, action_bound, 1e-4, tau, batch_size, \n",
    "                                          predictor_type, use_batch_norm)\n",
    "                    critic = TD3StockCritic(sess=sess, state_dim=state_dim, action_dim=action_dim, tau=1e-3,\n",
    "                             learning_rate=1e-3, num_actor_vars=actor.get_num_trainable_vars(),\n",
    "                             predictor_type=predictor_type, use_batch_norm=use_batch_norm,\n",
    "                               inp_actions=actor.scaled_out)\n",
    "                    actor_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(action_dim))\n",
    "\n",
    "                    model_save_path = get_model_path(dataset_name, framework, window_length, predictor_type, use_batch_norm, technical_indicators_flag)\n",
    "                    summary_path = get_result_path(dataset_name, framework, window_length, predictor_type, use_batch_norm, technical_indicators_flag)\n",
    "\n",
    "                    td3_model = TD3(None, sess, actor, critic, actor_noise, obs_normalizer=obs_normalizer, \n",
    "                                    log_return=log_return, config_file='config/stock.json', \n",
    "                                    model_save_path=model_save_path, summary_path=summary_path)\n",
    "                    td3_model.initialize(load_weights=True, verbose=True)\n",
    "                    models.append(td3_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights_list = []\n",
    "\n",
    "# In-sample Compiled Results\n",
    "for model_name in model_names:\n",
    "            \n",
    "    if technical_indicators_flag:\n",
    "        env = PortfolioEnv(history, assets, date_list, start_date=train_start_date, \n",
    "                              end_date=train_end_date, window_length=window_length, \n",
    "                                  technical_indicators_flag=technical_indicators_flag, \n",
    "                                  technical_indicator_history=ti_dict[window_length])\n",
    "    else:\n",
    "        env = PortfolioEnv(history, assets, date_list, start_date=train_start_date, \n",
    "                              end_date=train_end_date, window_length=window_length)\n",
    "\n",
    "    dates, _, _, _, model_weights, _, _, _ = test_model(env, models_subset)\n",
    "    model_weights_list.append(model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLPS\n",
    "env = PortfolioEnv(history, assets, date_list, start_date=train_start_date, \n",
    "                              end_date=train_end_date, window_length=3, olps=True)\n",
    "\n",
    "olps_model_names = ['CRP', 'BCRP', 'OLMAR', 'PAMR', 'RMR', 'WMAMR', 'EG', 'ONS', 'UP']\n",
    "observations_list, in_olps_portfolio_values_list, weights_list, dates = test_portfolio_selection_multiple(env, \n",
    "                                                                                                olps_model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_window = 1000 # Episode Steps\n",
    "roll_reward_df = pd.DataFrame()\n",
    "roll_reward_df[\"Date\"] = dates\n",
    "roll_reward_df.set_index('Date', inplace=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6), dpi=100)\n",
    "plt.title('Rolling Episode Reward (Window = {})'.format(rolling_window))\n",
    "plt.ylabel('Episode Reward')\n",
    "plt.xlabel('Day')\n",
    "\n",
    "\n",
    "for i in range(len(model_names)):\n",
    "    # Rerun to get Result list\n",
    "    _, _, _, _, reward_list = test_with_given_weights(env, model_weights_list[i])\n",
    "    roll_reward_df['day_rewards'] = np.append(np.repeat(np.nan, 0), reward_list)\n",
    "    roll_reward_df[model_names[i]] = roll_reward_df['day_rewards'].rolling(rolling_window).apply(sum, raw=True)\n",
    "    plt.plot(roll_reward_df[model_names[i]].iloc[rolling_window:], label=model_names[i], \n",
    "                 linewidth=1)\n",
    "\n",
    "for i in range(len(olps_model_names)):\n",
    "    # Rerun to get Result list\n",
    "    _, _, _, _, reward_list = test_with_given_weights(env, weights_list[i])\n",
    "    roll_reward_df['day_rewards'] = np.append(np.repeat(np.nan, 0), reward_list)\n",
    "    roll_reward_df[olps_model_names[i]] = roll_reward_df['day_rewards'].rolling(rolling_window).apply(sum, raw=True)\n",
    "    plt.plot(roll_reward_df[olps_model_names[i]].iloc[rolling_window:], label=olps_model_names[i], \n",
    "                 linewidth=1, \n",
    "                 linestyle='dashed')\n",
    "\n",
    "plt.xticks(np.arange(0, len(dates), rolling_window))\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(1.25, 1), shadow=True, ncol=1)\n",
    "plt.grid(color='black', linestyle=':', linewidth=1)\n",
    "plt.show()\n",
    "\n",
    "# Drop column\n",
    "roll_reward_df.drop('day_rewards', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roll_reward_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
